from typing import List
from timpani.content_sources.aws_s3_csv_content_source import AWSS3CSVContentSource
from timpani.raw_store.item import Item

import timpani.util.timpani_logger

logging = timpani.util.timpani_logger.get_logger()


class AWSS3CSVAAPITweetsContentSource(AWSS3CSVContentSource):
    """
    Implements specific details of accessing the AAPI tweets csv file
    Scott: The original data is from the Twitter API 1 year ago.
    The derived fields in the CSV are cluster_id, dataset, and COVID-19 category.
    """

    def get_source_name(self):
        """
        This knows how to handle a specific csv format of AAPI-related
        tweeets stored in s3
        """
        return "s3_csv_aapi_tweets"

    def process_csv_chunk(
        self, chunk: List[str], workspace_id: str, query_id: str, page_id: str
    ):
        """
        Assumes input is the page result extracted from csv
        and ready to be split by lines.
        Parse each item of the 'data' array into a Raw Store
        and return as a payload array. Trying to delay parsing
        until later in the pipeline, but do need id fields extracted.

        Examples:
        author_id,conversation_id,created_at,edit_history_tweet_ids,id,lang,possibly_sensitive,referenced_tweets,
        text,entities.urls,public_metrics.impression_count,public_metrics.like_count,public_metrics.quote_count,
        public_metrics.reply_count,public_metrics.retweet_count,entities.mentions,attachments.media_keys,in_reply_to_user_id,
        entities.annotations,entities.hashtags,attachments.poll_ids,geo.place_id,clean_text,cluster_id,cluster_size,
        is_retweet,dataset,covid_id,covid_cat,covid_prob

        1360964483632496648,1598828291661578240,2022-12-02T23:55:44.000Z,{1598828291661578240},1598828291661578240,zh,f,"[{""id"":
        ""1598827602403160064"", ""type"": ""quoted""}]",8. åˆ°2020å¹´ï¼Œé€£ç·šçš„åƒèˆ‡
        è€…åˆªé™¤æ¨æ–‡çš„è«‹æ±‚æ˜¯ä¾‹è¡Œå…¬äº‹ã€‚ ä¸€ä½é«˜ç®¡æœƒå¯«ä¿¡çµ¦å¦ä¸€ä½é«˜ç®¡ï¼šâ€œæ‹œç™»åœ˜éšŠçš„æ›´å¤š
        è©•è«–ã€‚â€ å›è¦†å°‡å›ä¾†ï¼šâ€œå·²è™•ç†â€ã€‚
        https://t.co/VnInqMdywy,"[{""display_url"":
        ""twitter.com/mtaibbi/status\u2026"", ""end"": 93, ""expanded_url"":
        ""https://twitter.com/mtaibbi/status/1598827602403160064"", ""start"":
        70, ""url"": ""https://t.co/VnInqMdywy""}]",0,8,0,0,3,NaN,,,NaN,NaN,,,8.
        åˆ°2020å¹´ï¼Œé€£ç·šçš„åƒèˆ‡è€…åˆªé™¤æ¨æ–‡çš„è«‹æ±‚æ˜¯ä¾‹è¡Œå…¬äº‹ã€‚ ä¸€ä½é«˜ç®¡æœƒå¯«ä¿¡çµ¦å¦ä¸€ä½
        é«˜ç®¡ï¼šâ€œæ‹œç™»åœ˜éšŠçš„æ›´å¤šè©•è«–ã€‚â€ å›è¦†å°‡å›ä¾†ï¼šâ€œå·²è™•
        ç†â€ã€‚,0,2,t,AAPI,8,Misrepresentation of Government
        Guidance,1.41577553749084

        1360964483632496648,1598827957325283328,2022-12-02T23:54:24.000Z,{1598827957325283328},1598827957325283328,zh,f,"[{""id"":
        ""1598825917165572099"", ""type"": ""quoted""}]",6. ç„¶è€Œï¼Œéš¨è‘—æ™‚é–“çš„æ¨
        ç§»ï¼Œè©²å…¬å¸è¢«è¿«æ…¢æ…¢å¢åŠ é€™äº›å£å£˜ã€‚ ä¸€äº›æœ€æ—©æ§åˆ¶è¨€è«–çš„å·¥å…·æ—¨åœ¨æ‰“æ“Šåƒåœ¾éƒµä»¶
        å’Œé‡‘èæ¬ºè©è€…ç­‰ã€‚ https://t.co/nd2vjPG02l,"[{""display_url"":
        ""twitter.com/mtaibbi/status\u2026"", ""end"": 80, ""expanded_url"":
        ""https://twitter.com/mtaibbi/status/1598825917165572099"", ""start"":
        57, ""url"": ""https://t.co/nd2vjPG02l""}]",0,4,0,0,4,NaN,,,NaN,NaN,,,6.
        ç„¶è€Œï¼Œéš¨è‘—æ™‚é–“çš„æ¨ç§»ï¼Œè©²å…¬å¸è¢«è¿«æ…¢æ…¢å¢åŠ é€™äº›å£å£˜ã€‚ ä¸€äº›æœ€æ—©æ§åˆ¶è¨€è«–çš„å·¥
        å…·æ—¨åœ¨æ‰“æ“Šåƒåœ¾éƒµä»¶å’Œé‡‘èæ¬ºè©è€…ç­‰ã€‚,1,2,t,AAPI,1,The Origins of
        COVID-19,1.19265651702881

        1360964483632496648,1598827801313873920,2022-12-02T23:53:47.000Z,{1598827801313873920},1598827801313873920,zh,f,"[{""id"":
        ""1598826284477427713"", ""type"": ""quoted""}]",7. æ…¢æ…¢åœ°ï¼Œéš¨è‘—æ™‚é–“çš„æ¨
        ç§»ï¼Œæ¨ç‰¹å“¡å·¥å’Œé«˜ç®¡é–‹å§‹ç‚ºé€™äº›å·¥å…·æ‰¾åˆ°è¶Šä¾†è¶Šå¤šçš„ç”¨é€”ã€‚ å±€å¤–äººé–‹å§‹è«‹é¡˜å…¬å¸
        ä¹Ÿæ“ç¸±æ¼”è¬›ï¼šé¦–å…ˆæ˜¯ä¸€é»ï¼Œç„¶å¾Œæ˜¯æ›´é »ç¹ï¼Œç„¶å¾Œæ˜¯ä¸æ–·ã€‚
        https://t.co/aRCVKeYsoL,"[{""display_url"":
        ""twitter.com/mtaibbi/status\u2026"", ""end"": 98, ""expanded_url"":
        ""https://twitter.com/mtaibbi/status/1598826284477427713"", ""start"":
        75, ""url"":
        ""https://t.co/aRCVKeYsoL""}]",0,11,0,1,2,NaN,,,NaN,NaN,,,7. æ…¢æ…¢åœ°ï¼Œéš¨
        è‘—æ™‚é–“çš„æ¨ç§»ï¼Œæ¨ç‰¹å“¡å·¥å’Œé«˜ç®¡é–‹å§‹ç‚ºé€™äº›å·¥å…·æ‰¾åˆ°è¶Šä¾†è¶Šå¤šçš„ç”¨é€”ã€‚ å±€å¤–äººé–‹
        å§‹è«‹é¡˜å…¬å¸ä¹Ÿæ“ç¸±æ¼”è¬›ï¼šé¦–å…ˆæ˜¯ä¸€é»ï¼Œç„¶å¾Œæ˜¯æ›´é »ç¹ï¼Œç„¶å¾Œæ˜¯ä¸
        æ–·ã€‚,2,3,t,AAPI,3,"The Nature, Existence, and Virulence of
        SARS-CoV-2",1.4522420167923

        1360964483632496648,1598826858711171072,2022-12-02T23:50:02.000Z,{1598826858711171072},1598826858711171072,en,f,"[{""id"":
        ""1598825403182874625"", ""type"": ""retweeted""}]",RT @elonmusk: Here
        we go!! ğŸ¿ğŸ¿,NaN,0,0,0,0,88358,"[{""end"": 12, ""id"": ""44196397"",
        ""start"": 3, ""username"": ""elonmusk""}]",,,NaN,NaN,,,: Here we go!!
        ğŸ¿ğŸ¿,3,29,t,AAPI,2,Transmission,1.33333361148834
        1360964483632496648,1598825308936880128,2022-12-02T23:43:53.000Z,{1598825308936880128},1598825308936880128,zh,f,NaN,"â¤ï¸ğŸ¤ğŸ’™
        äººæ°‘çˆ±å·çˆ±å¾—ç—´ç‹‚ğŸ˜‚

        """
        header = [
            "author_id",
            "conversation_id",  # this is the id of the thread
            "created_at",
            "edit_history_tweet_ids",
            "id",  # This is the individual tweet id
            "lang",
            "possibly_sensitive",
            "referenced_tweets",
            "text",
            "entities.urls",
            "public_metrics.impression_count",
            "public_metrics.like_count",
            "public_metrics.quote_count",
            "public_metrics.reply_count",
            "public_metrics.retweet_count",
            "entities.mentions",
            "attachments.media_keys",
            "in_reply_to_user_id",
            "entities.annotations",
            "entities.hashtags",
            "attachments.poll_ids",
            "geo.place_id",
            "clean_text",
            "cluster_id",
            "cluster_size",
            "is_retweet",
            "dataset",
            "covid_id",
            "covid_cat",
            "covid_prob",
        ]
        payload = []
        badrows = 0

        for row in chunk:
            # for row in csv.DictReader(chunk, fieldnames=header):
            # couldn't get the DictReadier to work, so diy

            # if there are lots of bad rows, give up
            assert badrows < 100

            if len(row) == 0:
                logging.info("skipping blank empty row")
                badrows += 1
                continue

            if len(row) != len(header):
                logging.info(f"row is missing columns: {row}")
                badrows += 1
                continue

            rowdict = {}
            for i in range(len(row)):
                rowdict[header[i]] = row[i]

            # pull out the tweet id from the data
            # and then append everything else as a json blob

            # logging.info(f"row: {row}")
            # logging.info(f"rowdict: {rowdict}")

            content_id = rowdict["id"]  # individual tweet id
            item = Item(
                run_id=self.run_state.run_id,
                workspace_id=workspace_id,
                source_id=self.get_source_name(),
                query_id=query_id,
                page_id=page_id,
                content_id=content_id,
                content=rowdict,
            )
            payload.append(item)
            if badrows > 0:
                logging.warning(
                    f"Skipped {badrows} rows that did not map correctly from page {page_id}"
                )
        return payload
